Introduction
Lacework platform components can be divided to the following five categories:

External services (like the one provided by Snowflake, Wavefront, Monitis, hub.docker.com, etc)
Services provided by AWS (like EC2, S3 storage, Kinesis, SQS, etc) 
Kubernetes cluster layer and related built-in components
System components created and managed by the Lacework DevOps team
Software components created and managed by the Lacework engineering team
This document provides a summary of all used platform components.

External services
Snowflake	Data warehouse service (SQL-like database)	Primary storage for raw agent data and all generated graphs/events/reports
Monitis	Web performance and availability monitoring service	External monitoring of Lacework UI and API's availability from several locations across the USA
Pagerduty	Alert notifications and escalation service	Dispatching of alerts generated by Wavefront and Monitis
Wavefront	Metrics collections and monitoring service	Aggregation of many metrics exposed by the Lacework application and system components, K8s cluster, AWS
hub.docker.com	Docker container registry service	Primary Docker container registry service for Lacework application and system containers
quay.io	Docker container registry service with vulnerability scanning functionality	Vulnerability scanning for Lacework application and system containers
shippable.com	Cloud-based job orchestration system	Building Lacework software components and Docker containers
Slack.com	Enterprise communication/chat service	Slack is widely used by Lacework for internal communication; in this specific case several Slack channels are used by different components to send informational and alert messages to the teams
Gmail	Enterprise email service	Many Lacework components are sending automated emails to different generic and specialized email groups
Services provided by AWS
VPC	Virtual Private Cloud	VPCs are used for isolation between management services, application components and Aurora RDS 
VPC Peers	IP-layer connections (routes) between different VPCs	Used to establish connectivity between different VPCs
SQS	A pay-per-use web service for storing messages in transit between computers	Developers use SQS to build distributed applications with decoupled components without having to deal with the overhead of creating and maintaining message queues.
Kinesis	Data streaming service	Streaming service for agent data (temporary storage for agent data before it is loaded to S3 (data path) and agent configuration information (control path)); Kinesis is also used by Threat Resolver to temporary store source data before uploading it to Snowflake.
S3	Object storage service	
See Used AWS S3 Buckets for details

Aurora RDS	MySQL database	Database for customer configurations (customers, users, properties, etc); does not include agent data or security events (which are stored in Snowflake).
Route 53	DNS service	
KMS	Key Management Service	
IAM Policies	

IAM Roles	

SNS	

DynamoDB	
The service is used indirectly by AWS Kinesis SDK (we don't manage DynamoDB manually or via Terraform code)
ASG	

ELB	Elastic Load Balancer (Layers 4 and 7)	The service is used in three places: 1) Kubernetes API end-point; 2) Kubernetes Bastion server SSH access; 3) HTTPS end-point to internal services controlled by "internal-nginx-ingress" system chart
NLB	Network Load Balancer (Layer 4)	The service is used to expose public end-points like Nginx-k8s service
EIP	

NAT Gateways	

Security Groups	

Kubernetes layer
Lacework is using Kops to manage its Kubernetes clusters.

Software components managed by the TechOps team
Redis	redis	Caching engine for IRIS and agentsrv	
Curator	curator	Configured as a daily Kubernetes cronjob; it deletes Elasticsearch indexes older than 7 days.	
Elasticsearch	es	Elasticserch cluster for storage of all Kubernetes logs shipped by fluentd component	
Cerebro	cerebro	Admin/monitoring UI for Elasticsearch cluster	
Elastalert	elastalert	Log monitoring and alerting tool for log data stored in the Elastsearch cluster	
Nginx LB	nginx-k8s	Primary HTTP/HTTPS load balancer between consumers (LW agents and portal/API users) and LW API service	
Fluentd	fluentd	The daemonset is responsible to collect logs from K8s and all running containers and ship them to the Elasticsearch cluster 	
Heapster	heapster	The process is responsible to collect node and pod usage data (CPU, memory, file system) exposed by Kubernetes and send the information to the local Wavefront proxy service	
Wavefront Proxy	wavefront-proxy	The service receives data from other local data collectors like heapster, telegraf and Lacework application containers, and forwards the data to the Wavefront cloud service	
Lacework Datacollector	
All Lacework production servers are running Lacework's data collection agents which report data to account lacework.lacework.net  	
Jenkins	
See Management of Jenkins Server	
Kubernetes Cluster Auto-scaler	cluster-autoscaler	The service is responsible to monitor a Kubernetes cluster for pods in Pending stage and automatically scale in/out relevant instance groups.	
Etcd cluster	etcd	etcd cluster used by many Lacework microservices	
Internal Nginx ingress controller	
internal-nginx-ingress

The service is used as a backdoor to access different application microservices in a cluster	
Reporter of Kubernetes state metrics	kube-state-metrics	

Reporter of pod metrics	metrics-service	

Custom K8s cronjob to perform hourly Aurora RDS snapshots	rds-backup	See Management of Aurora RDS "airflow" MySQL database	
Telegraf	telegraf	The component collects and reports a lot of monitoring metrics	
Kibana	Kibana	See How To Be The Best Friends With Kibana	
Assignment of IAM roles to pods	KIAM	See KIAM Management FAQ	
OpenVPN	
See OpenVPN Server Deployment Procedure (Outdated)	
Software components managed by the engineering team


Team	Component	
Kubernetes Chart Name

Service Description	
Primary

Owner

Secondary

Owner

Contributor


Agent

Agent	
A software component deployed on customer servers and responsible to send security data to Lacework's cloud service (the entry point is agentsrv)	Rakesh	Alok	
Agent-metadata-loader	agent-metadata-loader	Loads agent debug stats to the <ENVTYPE>_MDB database. Reads data from <ENVTYPE>-agentmgr kinesis stream	Rakesh	Alok	














IRIS










Reporting	reporting	Batch job:  It takes alerts generated by EventGen and translates them to a meaningful format which later can be translated to email or Slack message by Alert Notification Manager; it uses SQS to communicate with Alert Notification Manager	 Jeff	James J	
Alert Notification Manager	alert-notification-mgr	Batch job: Is triggered by SQS events created by Reporting module; it takes information about an alert and generate human-readable message in format depending on target communication channel (email, Slack, etc) and sends them out via customer-defined integration, severity levels, etc	Jeff	James J	
File Converter	file-converter	HTML to PDF converter in case we need to send PDF by email; used by Reporting module which stores input data in S3 and expects final PDF in S3 too (used open source tool: wkhtmlTOpdf)	Jeff	James J	
S3 Exporter	s3-exporter	Daily export from SF to S3	Aaron	Mobeen	
Rainbow	rainbow	Customer portal UI application (static files to be loaded into the browser); hosted by IRIS service	Kinjal	Ivan	
Iris	rainbow	Customer portal API component (provides customer API and hosting for static IRIS files)	Aaron	Jeffrey	
Electra-Internal-Portal	rainbow	Used by IRIS and Reporting to generate and render compliance reports.	Aaron	Jeffrey	
Api-Server	rainbow	Used by external callers to query event and compliance APIs	Aaron	Mobeen	
azure-cfg-cli	azure-cfg-cli	Cli to create an azure cfg integration (the container is provided to Lacework customers)	


























Ingestion

Agentsrv	agensrv	
Receives data from agents deployed on customers servers; manages agent upgrades; receives configuration and debug information from agents; stored received agent data to a Kinesis stream

Joe Cao	Swathi	Krishnan, Siyuan
AuthServer	auth-server	Token manager for agents; used by AgentSrv	Krishnan	Tasos	
S3 Loader	s3-loader	Reads agent data from Kinesis and stores in S3 bucket	Tasos	Krishnan	
DB Loader	db-loader	
Reads agent data from S3 and loads it to SF

Tasos	Krishnan	
ETL History Loader	etl-history-loader	Receives messages via SQS from ETL pipeline (S3 Loader and DB Loader) about processed raw data LSN (Load Sequence Number) and stores it in SF; used in case of ETL troubleshooting 	Swathi	Krishnan	
AWS/GCP/Azure Config Analyzer	aws-cfg-analyzer	The module is triggered by three sources (Hawkeye as a daily job, on initial customer AWS Config integration and every time a portal user is clicking on "Run Report" button in the UI) and it does the following: reads customer's AWS configuration, runs CIS report analyzer and stores the report in SF; as it runs the module also generates compliance change events comparing to the previous report 	
Krishnan


Divyang
AWS/Azure/GCP

Cloud Activity Analyzer

aws-cloudtrail-analyzer

gcp-at-analyzer

azure-al-analyzer

A standalone data collection module which listens to customers' SQS queues used by AWS CT integration to process new CT logs generated by AWS (the source logs are present in customers' S3 bucket); as it runs it generates CEP events; the module does aggregation of raw data and the results (like a new AWS user created in customer account) are stored in SF.	
Krishnan

Tasos	
Config Data Collector	cfg-data-collector	
Reads request messages from a global queue and pulls configuration information (i.e. calls a list of APIs) for the given cloud integration. Stores the complete Json results to s3 buckets and then sends a message to a per customer completion queue. 

Swathi	Tasos	Kamal
Config Data Collector Scheduler	cfg-data-collector-scheduler	On an hourly basis, finds all existing AWS integrations and adds request messages to the global queue for each integration. These messages are picked up by config data collector. 	Swathi	Tasos	Kamal
Queue Loader	queueloader	Picks data from S3 buckets (for example, data downloaded by Threat Aggregator) and loads it to SF; Queue Loader is receiving an API call from Threat Aggregator about new data in S3	Tasos	Swathi	Mandar














Modeling	Hawkeye	hawkeye	Task scheduler	Divyang	
Harish
EventGen	eventgen	Batch job: Generates customer alerts about detected anomalies; it takes data from GBM, CT, third-party sources, etc; it qualifies a signal by severity.	Ashish	
Mandar
GBM	gbm	Batch job: Polygraph computation process for all models (primary CPU and RAM consumer of the whole madhouse); it also computes differences (observations) between graphs for different hours (time slots)	Kamal, Quentin	
Harish
GBM Runner	gbm-runner	Batch job: A wrapper for GBM: it receives job requests from Hawkeye, pulls necessary data from SF and other sources, sends the data to a GBM process for actual processing, receives the results and stores them in SF	
Kamal, Quentin


Harish
Threat Resolver	threat-resolver	Accepts and answers queries from GraphGen, EventGen and other components about specific IP addresses, MD5 file hashes, reverse DNS records, etc.  Threat Resolver uses two types of data sources to answer the queries: information from third-party feeds regularly pulled by Threat Aggregator and direct calls to external sources like WHOIS, Reversing Lab (SHA256 file hashes)	Ashish	

GraphGen	graphgen	Batch job: Takes raw information about connections/processes/users/ports/etc and tries to connect all possible dots (creates nodes and edges) in the realm of a customer's environment; the result of the job is a first-level physical graph which later consumed by GBM to build a polygraph; the job also uses TFIDF algorithm to define common names for host names, machine tags and command line parameters	Kamal 	
Harish
SSHTracker	ssh-tracker	Batch job: The job maps processes to users; it tracks real/effective user ID changes and also remote user access from server to server	Kamal	
Harish
batch-tester	batch-tester	(non prod environments) - Automation testing component (intended for batch pipeline components)	
Sagar


Harish
Container Repository Poller	cr-poller	Pulls Repository, images tags and manifest information and tracks changes in repositories to detect new images.	Divyang	

Vuln Scanner	vuln-scanner	
Divyang	

Vuln Scan runner	vuln-scan-runner	
Divyang	

Threat Aggregator	threataggr	(prod only) - Daily batch job managed by Hawkeye. Pulls data from third-party sources like IP address credibility lists, reverse DNS dataset, geo IP mapping; it stores downloaded data to an S3 bucket. Some jobs are executed monthly and data is submitted directly to SF.	Divyang	Ashish	Harish








Analytics


Data Sanity	data-sanity	
Joe	Aaron	
Custom Rules/Policies	custom-rule-executor	Batch job: Takes the custom and default rule definitions from UI, which is picked from Electra by this container, generates and executes SQL on Snowflake to generate observations that eventually makes into event.	Mandar	Bharath	
Container Vulnerability Evaluator	vuln-evaluator	Container that generates evaluations based on scan results	Hank	
Divyang/

Bharath

Ashish
DB Manager	db-mgr	Schema manager for SF; used internally to change DB schema during initial deployments and software upgrades	Mandar	

Snowflake Manager	
1) The module regularly checks Snowflakes for long-waiting table locks and releases them after configurable time (normally 30 minutes);

2) The module also provides an interface to check and change the size of a Snowflake warehouse (the feature is used by "ops" container)

Swathi??	

Hubble	N/A	This JavaFX desktop application is used for data investigation and visualization. This program is not a micro-service.	
Joe	
LWBI	lwbi-exporter	
Exporter: component performs regularly scheduled data export for Lacework Business Intelligence
New cards and data definitions
Hank

???



Query Service/

ShadowQS

query-service	Used by IRIS to query customer data from SF and Aurora (internal components are using ShadowQS service)	Siyuan	Mandar, Joe	Tasos
QSJobServer	qsjobserver	Batch job: Pre-aggregates some events data for Query Service (kinda caching - MV); the results are stored in SF 	Mandar	
Mandar/

Bharath/Joe


Metadata library	metadata-library	Library to handle all metadata-related requests, such as fetching objects from database	Siyuan	
Siyuan
LQL Evaluator	lql-evaluator	Component that performs Evaluations for LQL (custom defined rules in Lacework custom language)	Hank	Joe	Siyuan

DevOps	Task Runner	task-runner	The multi-purpose service is responsible to several activities: manipulations with Hawkeye/GBM states, collection of customer and batch pipeline metrics, etc	Arpit	

Snowflake Autoscaler	snowflake-autoscaler	The component is responsible for automatic vertical scaling of Snowflake warehouses	Arpit	

Deprecated	Usage Recorder	usage-recorder	Billing component for AWS Marketplace (prod only) - ask Vikram/Murat . -→ Murat to deprecate	


AWS SM Gateway	
amazon-sm-gwy

Billing component for AWS Marketplace (prod only) -→ Murat to deprecate	
