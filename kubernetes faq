What is "K8s"?
"K8s" is a short name for Kubernetes.

What is Helm?
Helm is a Kubenetes package manager. All Lacework components (modules) are configured as Helm packages ("charts" in Helm's terms). The configuration of the charts is located in directory "lacework/charts/".

What is "kubectl"?
"kubectl" is Kubenetes' primary management CLI tool.

How can I get all the tools mentioned in the document?
See Configuration of Kubernetes client environment document.

What is a deployment?
"Deployment" is one of many ways how a container can be deployed in a K8s cluster. All Lacework services (charts) are running as K8s deployments. 

What is a pod?
In case of Lacework, a pod is a single container running on a Kubernetes node (worker) (in general K8s supports running multiple containers but we use the functionality only for "nginx-k8s" pod which runs "nginx-k8s" and "agentsrv" containers). One deployment can have several pods (container instances). Several pods can run on one K8s node.

How to get access to a cluster?
Follow this procedure: Configuration of Kubernetes client environment

What is a "label" and its format?
A "label" is a way to version Docker containers published to hub.docker.com (and quay.io) by automatic Shippable and manual build jobs for Lacework application and system components. Each unique label marks a separate version of a published container.

Normally label strings are generated automated by container build jobs and have the following format:

[YYYY-DD-MM]_[BRANCH]_[COMMIT_ID]
An example of a label generated for a master branch:

2019-01-12_master_e3eb1a2



An example of a label generated for a branch for release version 2.01:

2019-01-15_v2.01_8fe0422

How to deploy a new label in a specific devtest environment?
The procedure will not work for "nginx-k8s" and "agentsrv" labels - you can upgrade them manually by modifying the labels file (please see comments in the file).



Use script "lacework/upgrade.sh" in "k8s" repo.

./upgrade.sh <ENV> <CHART> <NEW_LABEL>


The procedure:

Make sure that you are in "devtest" branch (otherwise your changes will be not picked by Jenkins)
While in the directory "lacework/" run command "upgrade.sh" with three expected parameters: environment name, chart name and a new label. Review planned changes and answer "yes" if you want to continue. For example: 

$ ./upgrade.sh qa5 gbm 2018-07-09_master_73bd52f
Mon Jul 9 13:46:16 PDT 2018 INFO Checking parameters...
Mon Jul 9 13:46:16 PDT 2018 INFO Pulling latest updates from Github...
Already up to date.
Mon Jul 9 13:46:17 PDT 2018 INFO Are you sure you want to upgrade chart gbm in environment qa5 to label 2018-07-09_master_73bd52f?
Only 'yes' answer will allow you to continue:yes
Mon Jul 9 13:46:18 PDT 2018 INFO Generating new labels file labels/lw-labels.qa5.new...
Mon Jul 9 13:46:18 PDT 2018 INFO Verifying new labels file...
Mon Jul 9 13:46:18 PDT 2018 INFO Verification completed
Mon Jul 9 13:46:18 PDT 2018 INFO Commiting the changes to Github...
[devtest 86e94d7] Set label 2018-07-09_master_73bd52f for chart gbm
 1 file changed, 1 insertion(+), 1 deletion(-)
Counting objects: 5, done.
Delta compression using up to 4 threads.
Compressing objects: 100% (5/5), done.
Writing objects: 100% (5/5), 489 bytes | 489.00 KiB/s, done.
Total 5 (delta 3), reused 0 (delta 0)
remote: Resolving deltas: 100% (3/3), completed with 3 local objects.
To github.com:/lacework/k8s
   22aba27..86e94d7  devtest -> devtest
Mon Jul 9 13:46:23 PDT 2018 INFO Successfully updated Github with new label 2018-07-09_master_73bd52f for chart gbm in environment qa5
$
Wait for a few minutes while your changes will be picked up by Jenkins and deployed in the target environment. Monitor the following Slack channels for automatic notifications about applied changes:
"devtest" channel for all QA*, DEV* and PREPROD* environments
"devops" channel for all PRODN* production environments
An example of a Slack notification about a modified chart:



For any reason, if the Jenkins job fails to execute with the new commits then the following Slack notification would be triggered with the details of the error.



Internally the upgrade.sh script is doing the following:

Pull recent changes in Github
Update a proper lw-labels file in "labels/" directory
Commit and push your changes back to Github
How to update a label for a component in multiple environments at the same time?
Use script lacework/upgrade-many.sh in K8s repo.

./upgrade-many.sh <CHART> <NEW-LABEL> <ENVNAME-1> <ENVNAME-2(Optional)> ... <ENVNAME-N(Optional)>
This script would update the new label for the specified component in the Labels file for respective environments. You would need to manually push these changes to git repo for the changes to reflect in K8s environments using Jenkins.

What is Jenkins?
From Jenkins' Wikipedia page: "Jenkins is an open source automation server written in Java. Jenkins helps to automate the non-human part of the software development process, with continuous integration and facilitating technical aspects of continuous delivery."

We use Jenkins to automate the deployment process of new changes introduced to "devtest" branch of "k8s" repo. It works like the following:

Changes are committed to Github
"devtest" Jenkins instance is configured with several deployment jobs (one job per managed environment) which are synchronizing Github configuration files with what is actually running in the target environments. Examples of Jenkins deployment jobs:
"DEV2_Update_Lacework_Charts" (deployment job for DEV2 env)
"QA5_Update_Lacework_Charts" (deployment job for QA5 env)
etc
The jobs are idempotent  
Jenkins is checking for new Github updates every few minutes, and once a change is detected it triggers all configured "*_Update_Lacework_Charts" jobs (even if new changes may be relevant only for one target environment)
The deployment jobs are executed and full reports (console outputs) are emailed to devops@lacework.net. If a job fails the console output is also emailed to the originator of the relevant commit to the repo.
The idea here is to keep all "devtest" environments always synchronized with "devtest" branch of "k8s" repo, and have only one way to make changes in the environments - via code committed to the Github repo.

How to update the configuration of PREPROD environment?
The configuration of PREPROD environment is managed a bit different comparing to "devtest" environments DEV* and QA*.

Configuration for "preprod1" environment (which is a Kubernetes side of PREPROD env) is managed in branch "preprod" of "k8s" repo
Jenkins job "PREPROD_Update_Lacework_Charts" is configured to monitor changes in "preprod" branch of "k8s" repo and automatically apply them to "preprod1" environment 
Changes should be applied to "devtest" branch and after that changes from "devtest" to "preprod" branches should be applied using PRs and careful code reviews by QA and TechOps teams


Never make changes directly in "preprod" branch. Always modify "devtest" branch first and after that PR the changes to "preprod" branch.



What's the policy of management of branches in "k8s" repo?
"devtest" branch is the primary working branch for the repo. All Lacework app related changes in the branch are automatically applied by Jenkins to the following environments:
All DEV* envs
All QA* envs 
"preprod" branch is used to manage configuration for PREPROD environment 
"master" branch is what is eventually applied to the production environment:
No changes are applied directly to the branch
All new changes are first tested on "devtest" and "preprod" branches (and corresponding environments), and after that PR'ed to the "master" branch to be careful reviewed and approved by QA and TechOps teams
"prodn1" and "prodn2" branches hold code which is automatically applied to "prodn1" and "prodn2" production environments
What files in "k8s" repo can be modified by developers/QA staff?
Software developers (including QA engineers) are allowed to directly modify and commit in "devtest" branch (but no other mentioned below restricted branches like "preprod", "prodn1", etc) the following changes:

Component labels in lacework/labels/lw-labels.* files for QA* and DEV* envs
Component resources specified in lacework/resources/resources.json.* files for QA* and DEV* envs
Component environment variables specified in lacework/env-variables/env-variables.json.* files for QA* and DEV* envs
The QA team is allowed to change the mentioned files for PREPROD* environments.

If developers/QA need to make changes in other files in the "k8s" repo (like chart templates, Terraform code, etc) the changes should be submitted as Pull Requests (PRs) to be reviewed, approved and merged by the TechOps team (Victor). Developers are not allowed to directly modify files besides mentioned above.



How to change the number of running pods (instances) for a chart?
Modify file "lacework/resources/resources.json.<env_name>" and commit changes to the repo - in a few minutes Jenkins will automatically update all affected clusters.

How to change memory and CPU allocation for a pod?
Modify file "lacework/resources/resources.json.<env_name>" and commit changes to the repo - in a few minutes Jenkins will automatically update all affected clusters.

What is Horizontal Pod Autoscaler (HPA) and which components are using it?
The Horizontal Pod Autoscaler automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics). 

In Lacework k8s environments, we have implemented observed CPU utilization metric based HPA. This helps in automatic scaling the pods out or in based on the load on the respective components. The following components are currently using the CPU utilization based HPA -

Rainbow
Agentsrv
Nginx-k8s
S3-loader
Aws-cloudtrail-analyzer
Aws-cfg-analyzer
Query-service
Shadow-qs
How to control HPA behavior?
HPA is configured using Helm charts for individual components. The behavior of HPA could be controlled by updating the values of the HPA parameters in the lacework/resources/resources.json.<env_name> file in the k8s repo for the respective environment and component. Commit the changes to Git repo and Jenkins jobs will update the components to reflect the change. 

"qa6" : {
  "agentsrv":         { "cpus" : 0.10, "mem" : 1000,   "instances" : 1,  "hpa_min_instances" : 1,  "hpa_max_instances" : 2,  "hpa_cpu_threshold" : 50 },
What is CPU requested time and how to manage it?


The unit suffix m stands for “thousandth of a core,” so this resources object (in the example image above) specifies that the container process needs 50/1000 of a core (5%) and is allowed to use at most 100/1000 of a core (10%). Likewise, 2000m would be two full cores, which can also be specified as 2 or 2.0. This value is set in the lacework/charts/<chartname>/values.yaml file for each and every chart. However, we do not directly update the values.yaml file, instead we update the lacework/resources/resources.json.<env_name> file which is converted to the values.yaml file by lacework/generate_value.sh script for respective charts. 

What is the use of hpa_cpu_threshold parameter in the lacework/resources/resources.json.<env_name> file?
The hpa_cpu_threshold parameter can be used to set the value of observed CPU utilization for a pod at which HPA would need to start another pod to share the load of the current pod for the respective component. This parameter is also controlled in the lacework/resources/resources.json.<env_name> file. 



How to temporarily disable HPA for a chart?

To disable HPA for any of the components for which it is enabled you need to simply set the value of "hpa_max_instances" parameters as "0" in the lacework/resources/resources.json.<env_name> file in the k8s repo for the respective environment and component. Commit the changes to Git repo and Jenkins jobs will update the components to reflect the change. 

How to receive notifications that HPA is at its max limit?
Monitoring for K8s HPA is configured in Wavefront. It will trigger a notification when the Autoscaling would cross the threshold of 90%. This means that when the ratio of the number of current pods and the number of maximum allowed pods would reach 90%, an alert would be triggered. Apart from this, the wavefront charts could also be proactively monitored for any significant changes in the state of HPA.

How to troubleshoot HPA?
In order to check the information about HPA following commands could be executed in any k8s environment -

kubectl get hpa
ip-10-74-101-46:~ ubuntu (K8s qa6) $ kubectl get hpa
NAME                      REFERENCE                            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
agentsrv                  Deployment/agentsrv                  2%/50%    1         2         1          4d
aws-cfg-analyzer          Deployment/aws-cfg-analyzer          0%/50%    1         2         1          4d
kubectl describe hpa <hpa-name>
ip-10-74-101-46:~ ubuntu (K8s qa6) $ kubectl describe hpa agentsrv
Name:                                                  agentsrv
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Wed, 16 Jan 2019 10:36:26 +0000
Reference:                                             Deployment/agentsrv
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  2% (2m) / 50%
Min replicas:                                          1
Max replicas:                                          2
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:           <none>
How to manage container environment configurations for different environments?
Modify file "lacework/env-variables/env-variables.json.<env_name>" and commit changes to the repo - in a few minutes, Jenkins will automatically update all affected clusters.

Please note that environment variable settings in "deployment.yaml" file (see an example below) will take precedence over settings in "env-variables/env-variables.json.<env_name>" file.

env:
  - name: prodDomain
    value: "true"
  - name: QS_PORT
    value: "10031"
  - name: REDIS_PORT
    value: "10030"
If an environment variable is set in "deployment.yaml" file of a chart it will be automatically set for the chart for all Lacework environments (production, preprod and devtest) so be very careful when making changes in "deployment.yaml" file. If the variable should have different values for different Lacework environments you need to do the following:

Remove the variable from "deployment.yaml" file of the chart
Add the variable (with different values) to all environments mentioned in "env-variables/env-variables.json.<env_name>" file
Ask to review your changes before committing them to "devtest" branch
How to get a list of running deployments?
Use "kubectl get deploy" command. For example:

$ kubectl get deploy
NAME                      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
agentsrv                  2         2         2            2           1h
alert-notification-mgr    1         1         1            1           1h
amazon-sm-gwy             0         0         0            0           1h
auth-server               1         1         1            1           1h
aws-cfg-analyzer          1         1         1            1           1h
aws-cloudtrail-analyzer   1         1         1            1           1h
db-loader                 1         1         1            1           1h
db-mgr                    1         1         1            1           1h
etl-history-loader        1         1         1            1           1h
eventgen                  1         1         1            1           1h
file-converter            1         1         1            1           1h
gbm                       1         1         1            1           1h
gbm-runner                1         1         1            1           1h
graphgen                  1         1         1            1           1h
hawkeye                   1         1         1            1           1h
nginx-k8s                 1         1         1            1           1h
ops                       1         1         1            1           1h
qsjobserver               1         1         1            1           1h
query-service             2         2         2            2           1h
queueloader               1         1         1            1           1h
rainbow                   1         1         1            1           1h
redis                     1         1         1            1           1h
reporting                 1         1         1            1           1h
s3-loader                 2         2         2            2           1h
shadow-qs                 1         1         1            1           1h
snowflake-mgr             1         1         1            1           1h
ssh-tracker               1         1         1            1           1h
threat-resolver           1         1         1            1           1h
threataggr                1         1         1            1           1h
usage-recorder            0         0         0            0           1h
$


How to get a list of running pods?
Use "kubect get pods" command. For example:

$ kubectl get pods
NAME                                       READY     STATUS    RESTARTS   AGE
agentsrv-5f449d974c-kmd4r                  1/1       Running   0          1h
agentsrv-5f449d974c-vw54k                  1/1       Running   0          1h
alert-notification-mgr-798765d47-tm92v     1/1       Running   0          1h
auth-server-744799d76c-88p5f               1/1       Running   0          1h
aws-cfg-analyzer-58585d4c7-gr9hv           1/1       Running   0          1h
aws-cloudtrail-analyzer-66d8f9855c-v6fh4   1/1       Running   0          1h
db-loader-74866f7cc-nbc8t                  1/1       Running   0          1h
db-mgr-9d7476fb-25jvk                      1/1       Running   0          1h
etl-history-loader-869c5f65b7-nbbpz        1/1       Running   0          1h
eventgen-5bbf84d74-tmmzx                   1/1       Running   0          1h
file-converter-76865545b6-m9ljw            1/1       Running   0          1h
gbm-9dd647d88-h5bgb                        1/1       Running   0          1h
gbm-runner-85dbd6d6f8-hnpq6                1/1       Running   0          1h
graphgen-66b594d75-zqx2z                   1/1       Running   0          1h
hawkeye-55f5db4c6c-7slsv                   1/1       Running   0          1h
nginx-k8s-66d4d6c554-gqvzg                 1/1       Running   0          1h
ops-658d8bd96f-ktrpg                       1/1       Running   0          1h
qsjobserver-84f7747499-5rdlt               1/1       Running   0          1h
query-service-bd4588d98-gbdvn              1/1       Running   0          1h
query-service-bd4588d98-w4xdn              1/1       Running   0          1h
queueloader-6676856d96-vctxp               1/1       Running   0          1h
rainbow-7dbcf4965d-hpkm8                   1/1       Running   1          1h
redis-56bdb699b-lvrqk                      1/1       Running   0          1h
reporting-7bf48989cc-7f5t8                 1/1       Running   0          1h
s3-loader-8cdcc7475-lgcjg                  1/1       Running   0          1h
s3-loader-8cdcc7475-spjc7                  1/1       Running   0          1h
shadow-qs-668cf56d96-lf754                 1/1       Running   0          1h
snowflake-mgr-57498fdd5-5p9sj              1/1       Running   0          1h
ssh-tracker-b46f5dbb7-hjft9                1/1       Running   0          1h
threat-resolver-f8b8588b8-9rbxn            1/1       Running   0          1h
threataggr-55f775f56-b7p8c                 1/1       Running   1          1h
$


How to get a list of nodes used by running pods?
Use script "list_pods_with_nodes.sh".

How to see current memory/CPU limits for a pod?
Use "kubectl describe pod <pod>" command and look for "Limits" section. For example:

$ kubectl describe pod eventgen-5bbf84d74-cbv6h
Name:           eventgen-5bbf84d74-cbv6h
Namespace:      default
Node:           ip-10-130-63-44.us-west-2.compute.internal/10.130.63.44
Start Time:     Mon, 09 Jul 2018 13:26:27 -0700
Labels:         app=eventgen
                pod-template-hash=166940830
Annotations:    <none>
Status:         Running
IP:             100.120.179.218
Controlled By:  ReplicaSet/eventgen-5bbf84d74
Containers:
  eventgen:
    Container ID:   docker://770ce7e1904dedd76b21e123ba910eef4ed829e8f085fb8a22ddd19dc6d70477
    Image:          lacework/eventgen:2018-07-06_v1.71_7e284fd
    Image ID:       docker-pullable://lacework/eventgen@sha256:5289b7e0591a6da649345efa05f51b22effd0dc3f6f3bf5d3c19ae49cc624835
    Port:           8089/TCP
    Host Port:      0/TCP
    State:          Running
      Started:      Mon, 09 Jul 2018 13:26:28 -0700
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     100m
      memory:  2000Mi
    Requests:
      cpu:     100m
      memory:  2000Mi
    Environment Variables from:
      eventgen-env-autogenerated              ConfigMap  Optional: false
      eventgen-env-marathon-app-resource-mem  ConfigMap  Optional: false
    Environment:
      WAREHOUSE_POOL:  devtest_batch
      HOST:             (v1:spec.nodeName)
    Mounts:
      /etc/lacework from lacework (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-62g6v (ro)
Conditions:
  Type           Status
  Initialized    True
  Ready          True
  PodScheduled   True
Volumes:
  lacework:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  lacework-secrets
    Optional:    false
  default-token-62g6v:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-62g6v
    Optional:    false
QoS Class:       Guaranteed
Node-Selectors:  lw-role=nodes-worker
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type    Reason                 Age   From                                                 Message
  ----    ------                 ----  ----                                                 -------
  Normal  Scheduled              44m   default-scheduler                                    Successfully assigned eventgen-5bbf84d74-cbv6h to ip-10-130-63-44.us-west-2.compute.internal
  Normal  SuccessfulMountVolume  44m   kubelet, ip-10-130-63-44.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume "default-token-62g6v"
  Normal  SuccessfulMountVolume  44m   kubelet, ip-10-130-63-44.us-west-2.compute.internal  MountVolume.SetUp succeeded for volume "lacework"
  Normal  Pulled                 44m   kubelet, ip-10-130-63-44.us-west-2.compute.internal  Container image "lacework/eventgen:2018-07-06_v1.71_7e284fd" already present on machine
  Normal  Created                44m   kubelet, ip-10-130-63-44.us-west-2.compute.internal  Created container
  Normal  Started                44m   kubelet, ip-10-130-63-44.us-west-2.compute.internal  Started container
$


A pod has the number of restarts higher than 0 (column RESTARTS). How to learn the reason of restart?
In the output of "kubectl describe pod" command look for information in section "Last State". For example:

$ kubectl describe pod query-service-bcfffb9fc-rc5ft
 
(removed many lines for clarity)
 
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
      Started:      Tue, 14 Aug 2018 14:04:48 -0700
      Finished:     Tue, 14 Aug 2018 15:09:36 -0700
    Ready:          True
    Restart Count:  2
 
(removed many lines for clarity)
 
$


How to see currently running labels in a cluster?
Use "./manage-helm.sh check_labels" command in "lacework/" directory. For example:

cd lacework
./manage-helm.sh check_labels


How to see logs from a specific pod?
You have two options:

Use "kubectl logs <pod>" command
Use "kubetail <pod>" command


How to see logs from all pods of a specific deployment?
Use "kubetail <name>" command. For example:

kubetail rainbow


"kubetail" internally spawns a separate instance of "kubectl" tool for every running pod of the deployment, and on MacOS computers the instances of "kubectl" will consume a lot of CPU resources (you will hear your CPU fan working louder).

How to see at once the logs from all Lacework pods?


kubetail --


How to see all error/warning messages?


kubetail -- | grep -v "closed connection" | grep -v INFO


How to see logs from X minutes/hours ago?
Use the "-s" option of "kubetail". For example:

# See last 30 minutes of logs for all rainbow containers
kubetail rainbow -s 30m
 
# See all db-loader logs for last 10 hours
kubetail db-loader -s 10h


How to see logs from the previous instance of the container in a pod?
Use the "-p" option of "kubectl logs" command. For example:

kubectl logs query-service-bcfffb9fc-rc5ft -p


How to copy a file to/from a pod?
The feature requires that the remote pod will have "tar" utility installed - otherwise the command will fail.



Use "kubectl cp" command. For example: 

# Copy file /usr/local/hawkeye/config/reset_handler.json from a Hawkeye pod to local dir /tmp/
kubectl cp hawkeye-55f5db4c6c-7slsv:/usr/local/hawkeye/config/reset_handler.json /tmp/
 
# Copy local file /tmp/reset_handler.json to a running Hawkeye pod
kubectl cp /tmp/reset_handler.json hawkeye-55f5db4c6c-7slsv:/usr/local/hawkeye/config/
How to access the Kibana service in order to browse application logs?
Each environment has its own instance of Kibana service
The service is protected with username "kibana" and password "rakesh"
Note that Kibana configuration (searches, visualizations, dashboards) is shared between all users of the service so be considerate of other users
Use URL like https://internal.<ENV>.corp.lacework.net/kibana (or https://internal.<ENV>.corp1.lacework.net/kibana for production) to access the service (VPN is required). The end-point is configured with a self-signed SSL certificate (you will need to accept the browser's security exception). For example, URL for dev2 environment:

For the list of internal DNS names for different environments, click here


https://internal.dev2.corp.lacework.net/kibana
What's the format of Kibana queries?
This is a good article with a lot of examples: https://www.elastic.co/guide/en/elasticsearch/reference/6.x/query-dsl-query-string-query.html

Also, see How To Be The Best Friends With Kibana page.

How environment and cluster names are organized?
Lacework environments are named "qa1", "qa2", "dev1", "prodn", etc
K8s clusters are named as environment names and suffix ".k8s.local"; for example: "qa5.k8s.local", "dev2.k8s.local", etc. 
How to see which clusters are configured in my working environment?
Use "ls" command to see a list of present config files in ~/.kube_keys/ directory. For example:

$ ls -la ~/.kube_keys/config-*
-rw-------  1 victorgartvich  staff  4144 Jul 17 14:00 /Users/victorgartvich/.kube_keys/config-dev2
-rw-------  1 victorgartvich  staff  4137 Jul 17 13:28 /Users/victorgartvich/.kube_keys/config-qa5
$
In this specific example, the computer is configured to access "dev2" and "qa5" environments.

How to see what's my current context (active cluster)?
Use script "whereami.sh" placed in tools/ directory. For example:

$ whereami.sh
Sat Jul 28 00:32:36 PDT 2018 INFO Getting environment information...
Sat Jul 28 00:32:36 PDT 2018 INFO Value of KUBECONFIG variable: /Users/victorgartvich/.kube/config
Sat Jul 28 00:32:37 PDT 2018 INFO Current Kubernetes cluster (context) is qa6.k8s.local
$


How to switch between clusters?
Use command "source lwenv <env>". Please note that "lwenv" script is located in "tools/" directory of "k8s" repo - make sure that the path is added to your PATH environment variable. For example: 

$ source lwenv dev2
INFO The shell has been configured to access Lacework environemnt dev2 (Kubernetes cluster dev2.k8s.local).
INFO Please remember the "source" the script in every shell you use to access the environment.
$


How can I access a TCP port exposed by a deployment or specific pod?
The recommended way is to use provided portforward.sh tool. The script accepts two parameters:

Lacework component (chart) name like "rainbow" or "query-service"
Local TCP port to listen on (a number over 1024)
The script will automatically learn the remote TCP port for the specified service.

For example:

$ portforward.sh query-service 7777
Thu Jul 26 15:03:12 PDT 2018 INFO Getting environment information...
Thu Jul 26 15:03:12 PDT 2018 INFO Processing configuration for env dev2, type devtest
Thu Jul 26 15:03:12 PDT 2018 INFO Checking that necessary utilities are installed on the local computer...
Thu Jul 26 15:03:12 PDT 2018 Remote port is not specified - trying to extract the information from the configuration of chart "query-service"...
Thu Jul 26 15:03:12 PDT 2018 INFO Running pre-flight checks...
Thu Jul 26 15:03:12 PDT 2018 INFO Checking that 'kubectl get pods' command is working...
Thu Jul 26 15:03:16 PDT 2018 INFO Checking that 'helm ls' command is working...
Thu Jul 26 15:03:18 PDT 2018 INFO Starting kubectl in port forwarding mode for deployment query-service, local port 7777, remote port 8080. Use Ctrl+C to stop the process...
Forwarding from 127.0.0.1:7777 -> 8080
Forwarding from [::1]:7777 -> 8080
$


You can use "kubectl port-forward deployment/<deployment> <local_port>:<remote_port>" command to establish a tunnel from localhost to a remote deployment. For example:  

# Connect to any pod in "query-service" deployment, listen on local port 8080 and remote pod port 8080
kubectl port-forward deployment/query-service 8080
 
# Connect to any pod in "query-service" deployment, listen on local port 8082 and remote pod port 8080
kubectl port-forward deployment/query-service 8082:8080


You can also use "kubectl port-forward <pod> <local_port>:<remote_port> format to connect to a specific pod. For example:

# Find a pod in rainbow deployment
$ kubectl get pods|grep rainbow
rainbow-7dbcf4965d-hpkm8                   1/1       Running   1          2h
$
 
# Proxy to the specific rainbow pod on local and remote ports 8888
kubectl port-forward rainbow-7dbcf4965d-hpkm8  8888
 
# Proxy to the specific rainbow pod on local port 8080 and remote ports 8888
kubectl port-forward rainbow-7dbcf4965d-hpkm8  8080:8888


How can I get inside a running pod?
Use "kubectl exec -it <pod> <command>" command. For example:

# Get a list of running db-loader pods
$ kubectl get pods|grep db-loader
db-loader-74866f7cc-nbc8t                  1/1       Running   0          2h
$
 
# Run bash shell inside pod db-loader-74866f7cc-nbc8t
$ kubectl exec -it db-loader-74866f7cc-nbc8t bash
root@db-loader-74866f7cc-nbc8t:/opt/db-loader# exit
$


Do I need to use a Bastion server to access a cluster?
No - you can manage a K8s cluster from your local computer (you still need to use a VPN).

How to add a new Chart (micro-service)?
Please see How To Add A New Micro-Service (Chart) document.

How can I learn more about Lacework's Kubernetes environments?
Read Kubernetes Engineering Description.



How can I stop a service (chart)?
Set the number of instances to 0 (in file resources/resources.json.<env_name>) and push your changes to Github. To resume a service set the instance count to the original value.

How can I restart a service?
Kill all running pods of the service using the command "kubectl delete pod" with the option "-l" (lowercase L) - the killed pods will be automatically restarted by Kubernetes. For example (the command will kill all "rainbow" pods):

kubectl delete pods -lapp=rainbow
Please note that the command will kill all running pods at once and may disrupt the service

How to manually stop (delete) a chart?
You SHOULD NOT do this manually. The environment's Jenkins job will automatically start the chart on the next run. If you want a service to stop running in an environment you should just set the service's "instances" count to 0 (zero).

Use "manage-helm.sh delete <CHART>" command. For example: 

manage-helm.sh delete rainbow
How to manually start a chart?
If a chart was previously stopped (deleted) using "manage-helm.sh delete" command then you can start it using "manage-helm.sh start <CHART>" command. For example:

manage-helm.sh start rainbow


How to manually upgrade a chart?
You SHOULD NOT do this. If you will change the configuration of a chart in an environment with settings which are different from what is committed in the environment's repository the environment's Jenkins job will automatically revert your changes on the next run.

Use "manage-helm.sh upgrade <CHART>" command. For example: 

manage-helm.sh upgrade rainbow
Any other resources about K8s troubleshooting?
See Kubernetes Troubleshooting Procedure.

I got under my control an existing development environment but it looks like it runs old labels... How can I update the environment?
Take a look at the Management Of Devtest Environments document.
